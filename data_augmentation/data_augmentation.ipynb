{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805e6b7",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a805e6b7",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0a0+0aef44c\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datasets as DS\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7576b5a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f7576b5a",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "  patch_size: patch_size for dataLoader\n",
    "  batch_size: batch number for each training epoch\n",
    "  workers: cores for loading the data, default 8\n",
    "return:\n",
    "  dataLoader: a dict with key 'train' 'val' 'test' for each phase in training and evaluating model\n",
    "'''\n",
    "def getDataSet(mode, patch_size, batch_size, workers=8):\n",
    "    # build a class to satisfy the input of loader producer provided by the paper\n",
    "    class Args:\n",
    "      dataset_path = \"/storage/data/classification_dataset_balanced/\"\n",
    "      patch_size = 1\n",
    "      batch_size = 1\n",
    "      workers = 1\n",
    "      mode = 0\n",
    "      def __init__(self, mode, patch_size, batch_size, workers):\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "        self.mode = mode\n",
    "    args = Args(mode, patch_size, batch_size, workers)\n",
    "    # use the loader producer from the paper\n",
    "    dataset = DS.CODEBRIM(torch.cuda.is_available(),args)\n",
    "    dataLoaders = {'train': dataset.train_loader, 'val': dataset.val_loader, 'test':dataset.test_loader}\n",
    "    return dataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b138a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "565b138a",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449"
    }
   },
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(EfficientNet, self).__init__()\n",
    "#     self.model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained = False)\n",
    "    # load efficient net from torchhub\n",
    "    self.model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet', type='efficientnet-widese-b0')\n",
    "    self.model.classifier[3] = nn.Linear(1280,6) #modify the output layer\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    x = torch.sigmoid(x) # the output from model should be fed into sigmoid to get the probability \n",
    "    return x\n",
    "  def _initialize_weights(self):\n",
    "    print(\"initialize parameters\")\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d):\n",
    "        #using kaiming's method to initialize convolution layer parameters as requested in the paper\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') \n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "      elif isinstance(m, nn.Linear):\n",
    "        #other parameters use normal distribution to initialize\n",
    "        nn.init.normal_(m.weight, 0, 0.01)  \n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 07:59:03,627][line: 39] ==> creating ./train_log/2021-12-07-07-59.log\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print training log -loss -accuracy -learning rate\n",
    "params:\n",
    "    output_dir: log output path\n",
    "return:\n",
    "    logger: logger.info() to print log into file\n",
    "'''\n",
    "def log_creater(output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    final_log_file = os.path.join(output_dir,log_name)\n",
    " \n",
    " \n",
    "    # creat a log\n",
    "    log = logging.getLogger('train_log')\n",
    "    log.setLevel(logging.DEBUG)\n",
    " \n",
    "    # FileHandler\n",
    "    file = logging.FileHandler(final_log_file)\n",
    "    file.setLevel(logging.DEBUG)\n",
    " \n",
    "    # StreamHandler\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    " \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(asctime)s][line: %(lineno)d] ==> %(message)s')\n",
    " \n",
    "    # setFormatter\n",
    "    file.setFormatter(formatter)\n",
    "    stream.setFormatter(formatter)\n",
    "\n",
    "     # addHandler\n",
    "    log.addHandler(file)\n",
    "    log.addHandler(stream)\n",
    " \n",
    "    log.info('creating {}'.format(final_log_file))\n",
    "    return log\n",
    "logger = log_creater(\"./train_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd029d5-5f95-4468-b44a-4dfd802e5504",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "bbd029d5-5f95-4468-b44a-4dfd802e5504",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\r\n",
    "import datasets as DS\r\n",
    "import random\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "#geometric transform\r\n",
    "#0->RandomHorizontalFlip\r\n",
    "#1->RandomVerticalFlip\r\n",
    "#2->RandomRotation\r\n",
    "#3->RandomResizedCrop\r\n",
    "#4->RandomPerspective\r\n",
    "geometric_transform_list = []\r\n",
    "RandomHorizontalFlip = transforms.Compose([\r\n",
    "    transforms.Resize(224),  # mandate\r\n",
    "    transforms.RandomHorizontalFlip(p = 1),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "geometric_transform_list.append(RandomHorizontalFlip)\r\n",
    "\r\n",
    "RandomVerticalFlip = transforms.Compose([\r\n",
    "    transforms.Resize(224),  # mandate\r\n",
    "    transforms.RandomVerticalFlip(p = 1),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "geometric_transform_list.append(RandomVerticalFlip)\r\n",
    "\r\n",
    "RandomRotation = transforms.Compose([\r\n",
    "    transforms.Resize(224),  # mandate\r\n",
    "    transforms.RandomRotation(360),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "geometric_transform_list.append(RandomRotation)\r\n",
    "\r\n",
    "RandomResizedCrop = transforms.Compose([\r\n",
    "    transforms.Resize(256),  # mandate\r\n",
    "    transforms.RandomResizedCrop(size = 224),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "geometric_transform_list.append(RandomResizedCrop)\r\n",
    "\r\n",
    "RandomPerspective = transforms.Compose([\r\n",
    "    transforms.Resize(224),  # mandate\r\n",
    "    transforms.RandomPerspective(distortion_scale=0.4, p = 1),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "geometric_transform_list.append(RandomPerspective)\r\n",
    "\r\n",
    "#photometric transform\r\n",
    "#0->GaussianBlur\r\n",
    "#1->RandomAdjustSharpness\r\n",
    "#2->Normalize\r\n",
    "photometric_transform_list = []\r\n",
    "\r\n",
    "GaussianBlur = transforms.Compose([#?torch.nn.Sequential\r\n",
    "    transforms.Resize(224),  # mandate\r\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "photometric_transform_list.append(GaussianBlur)\r\n",
    "\r\n",
    "RandomAdjustSharpness = transforms.Compose([#?torch.nn.Sequential\r\n",
    "    transforms.Resize(224),  # mandate\r\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor = 2, p=1),\r\n",
    "    transforms.ToTensor()])  # mandate\r\n",
    "photometric_transform_list.append(RandomAdjustSharpness)\r\n",
    "\r\n",
    "Normalize = torch.nn.Sequential(\r\n",
    "    transforms.Normalize((0.499, 0.559, 0.535), (0.021, 0.018, 0.019)))  \r\n",
    "Normalize = torch.jit.script(Normalize)\r\n",
    "photometric_transform_list.append(Normalize)\r\n",
    "\r\n",
    "#mode\r\n",
    "#-2->no transform\r\n",
    "#-1->randomly select a transform\r\n",
    "#0->RandomHorizontalFlip\r\n",
    "#1->RandomVerticalFlip\r\n",
    "#2->RandomRotation\r\n",
    "#3->RandomResizedCrop\r\n",
    "#4->RandomPerspective\r\n",
    "#5->GaussianBlur\r\n",
    "#6->RandomAdjustSharpness\r\n",
    "#7->Normalize\r\n",
    "def random_transform(input_batch, mode):\r\n",
    "    toPILTransform = transforms.ToPILImage()\r\n",
    "    toTensorTransform = transforms.ToTensor()\r\n",
    "    if(mode == -2):\r\n",
    "        return input_batch\r\n",
    "    i = 0\r\n",
    "    for tensor in input_batch:\r\n",
    "        img = toPILTransform(tensor)                \r\n",
    "        choice = mode\r\n",
    "        if choice == -1:#mix mode\r\n",
    "            choice = np.random.randint(0, high = 8)         \r\n",
    "        if(choice < 5):\r\n",
    "            input_batch[i] = geometric_transform_list[choice](img)\r\n",
    "        if(choice >=5 and choice < 7):\r\n",
    "            input_batch[i] = photometric_transform_list[choice-5](img)\r\n",
    "        if(choice == 7):\r\n",
    "            input_batch[i] = photometric_transform_list[2](tensor)\r\n",
    "        i = i + 1\r\n",
    "    return input_batch#torch.tensor(ret)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train a model according to the given parameters\n",
    "params:\n",
    "    root_dir: root_path\n",
    "    model: model for training\n",
    "    logger: return from log_creater()\n",
    "    lr_h: max(initial) learning rate\n",
    "    lr_l: min(final) learing rate\n",
    "    dataLoaders: return from getDataSet()\n",
    "    num_epochs: training epochs\n",
    "    resume: bool, save checkpoint every 10 epoch when the training interrupted, load checkpoint to resume\n",
    "    checkpoint: path for checkpoints, valid when resume == True\n",
    "    device: training device\n",
    "'''\n",
    "\n",
    "def train(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs = 300, resume=False, \n",
    "    checkpoint = None, device = \"cpu\"):\n",
    "    start_epoch = 1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr_h, momentum=0.9, weight_decay = 1e-5*0.1)\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=lr_l)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs, eta_min=lr_l)\n",
    "    best_acc_hard = 0.0\n",
    "    best_acc_soft = 0.0\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    save_path_hard = root_dir + '/hard.pth'\n",
    "    save_path_soft = root_dir + '/soft.pth'\n",
    "    iters = len(dataLoader['train'])\n",
    "    if resume:\n",
    "        path_checkpoint = root_dir + checkpoint  # checkpoint path\n",
    "        checkpoint = torch.load(path_checkpoint)  # load checkpoint\n",
    "        model.load_state_dict(checkpoint['net'])  # load model\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])  # load optimizer\n",
    "        start_epoch = checkpoint['epoch']  # load epoch\n",
    "        best_acc_soft = checkpoint['best_acc_soft']\n",
    "        best_acc_hard = checkpoint['best_acc_hard']\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):  # loop over the dataset multiple times\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint = {\n",
    "            \"net\": model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc_soft\": best_acc_soft,\n",
    "            \"best_acc_hard\": best_acc_hard\n",
    "            }\n",
    "            if not os.path.isdir(root_dir + \"/checkpoint\"):\n",
    "                os.mkdir(root_dir + \"/checkpoint\")\n",
    "            torch.save(checkpoint, root_dir + '/checkpoint/ckpt_best_%s.pth' %(str(epoch)))\n",
    "        #an epoch is divided into two phases train and evaluate\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_hard = 0\n",
    "            running_corrects_soft = 0\n",
    "\n",
    "            for i, sample in enumerate(dataLoaders[phase]):\n",
    "                inputs, labels = sample\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "                equality_matrix = (outputs.float() == labels).float()\n",
    "                hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "                soft = torch.mean(equality_matrix)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                #adjustment in scheduler\n",
    "                    scheduler.step(epoch + i / iters)\n",
    "        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_hard += hard.item()\n",
    "                running_corrects_soft += soft.item()\n",
    "\n",
    "            #calculate loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "            logger.info('{} Epoch:[{}/{}]\\t loss={:.5f}\\t acc_hard={:.3f} acc_soft={:.3f} lr={:.7f}'.format\\\n",
    "            (phase, epoch , num_epochs, epoch_loss, epoch_acc_hard, epoch_acc_soft, \\\n",
    "            optimizer.state_dict()['param_groups'][0]['lr'] ))\n",
    "\n",
    "            # deep copy the model\n",
    "            if  phase == 'val' and epoch_acc_hard > best_acc_hard:\n",
    "                best_acc_hard = epoch_acc_hard\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_hard)\n",
    "\n",
    "            if  phase == 'val' and epoch_acc_soft > best_acc_soft:\n",
    "                best_acc_soft = epoch_acc_soft\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_soft)\n",
    "    #evaluate\n",
    "    model = EfficientNet()\n",
    "    model.load_state_dict(torch.load(root_dir + '/hard.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"hard:--------------------------------------\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "    model.load_state_dict(torch.load(root_dir + '/soft.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"soft:--------------------------------------\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "evaluate a model\n",
    "params:\n",
    "    dataLoaders: return from getDataSet()\n",
    "    device: training device\n",
    "    model: trained model to evaluate\n",
    "    logger: return from log_creater()\n",
    "'''\n",
    "\n",
    "\n",
    "def evaluation(dataLoaders, device, model, logger):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    # calculate loss and accuracy for train data, evaluate data and test data\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        running_loss = 0.0\n",
    "        running_corrects_hard = 0\n",
    "        running_corrects_soft = 0\n",
    "\n",
    "      \n",
    "        for i, sample in enumerate(dataLoaders[phase]):\n",
    "            inputs, labels = sample\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "            equality_matrix = (outputs.float() == labels).float()\n",
    "            hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "            soft = torch.mean(equality_matrix)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects_hard += hard.item()\n",
    "            running_corrects_soft += soft.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "        logger.info(\"{}: loss:{:.5f} acc_soft:{:.3f} acc_hard:{:.3f}\".format(phase, epoch_loss, epoch_acc_soft, epoch_acc_hard))     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
     "kernelId": "50d3a52f-6e89-4eaf-a651-b10816833449",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      "5\n",
      "224\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 07:59:05,797][line: 20] ==> experiment on:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 07:59:05,798][line: 24] ==> batch_size:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 07:59:05,798][line: 25] ==> patch_size:224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 07:59:05,799][line: 26] ==> learning rate high:0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 07:59:05,799][line: 27] ==> learning rate low:1.5000000000000002e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/archive/torchhub.zip\" to /root/.cache/torch/hub/torchhub.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/efficientnet_widese_b0_pyt_amp/versions/20.12.0/files/nvidia_efficientnet-widese-b0_210412.pth\" to /root/.cache/torch/hub/checkpoints/nvidia_efficientnet-widese-b0_210412.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbfd9b43d2c47e1ae1e1e8ce05f53b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/32.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:07:29,377][line: 93] ==> train Epoch:[50/150]\t loss=0.06034\t acc_hard=0.908 acc_soft=0.979 lr=0.0000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I1207 08:07:29.377522 139766070904640 3841376463.py:93] train Epoch:[50/150]\t loss=0.06034\t acc_hard=0.908 acc_soft=0.979 lr=0.0000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:08:00,579][line: 93] ==> val Epoch:[50/150]\t loss=0.37260\t acc_hard=0.596 acc_soft=0.890 lr=0.0000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:08:00.579540 139766070904640 3841376463.py:93] val Epoch:[50/150]\t loss=0.37260\t acc_hard=0.596 acc_soft=0.890 lr=0.0000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:08:59,833][line: 93] ==> train Epoch:[51/150]\t loss=0.06135\t acc_hard=0.905 acc_soft=0.978 lr=0.0000493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:08:59.833852 139766070904640 3841376463.py:93] train Epoch:[51/150]\t loss=0.06135\t acc_hard=0.905 acc_soft=0.978 lr=0.0000493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:09:03,723][line: 93] ==> val Epoch:[51/150]\t loss=0.37218\t acc_hard=0.591 acc_soft=0.887 lr=0.0000493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:09:03.723223 139766070904640 3841376463.py:93] val Epoch:[51/150]\t loss=0.37218\t acc_hard=0.591 acc_soft=0.887 lr=0.0000493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:10:02,549][line: 93] ==> train Epoch:[52/150]\t loss=0.06348\t acc_hard=0.899 acc_soft=0.977 lr=0.0000984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:10:02.549632 139766070904640 3841376463.py:93] train Epoch:[52/150]\t loss=0.06348\t acc_hard=0.899 acc_soft=0.977 lr=0.0000984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:10:06,457][line: 93] ==> val Epoch:[52/150]\t loss=0.37453\t acc_hard=0.599 acc_soft=0.890 lr=0.0000984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:10:06.457987 139766070904640 3841376463.py:93] val Epoch:[52/150]\t loss=0.37453\t acc_hard=0.599 acc_soft=0.890 lr=0.0000984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:11:06,012][line: 93] ==> train Epoch:[53/150]\t loss=0.06319\t acc_hard=0.899 acc_soft=0.977 lr=0.0001668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:11:06.012797 139766070904640 3841376463.py:93] train Epoch:[53/150]\t loss=0.06319\t acc_hard=0.899 acc_soft=0.977 lr=0.0001668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:11:09,920][line: 93] ==> val Epoch:[53/150]\t loss=0.37531\t acc_hard=0.596 acc_soft=0.889 lr=0.0001668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:11:09.920840 139766070904640 3841376463.py:93] val Epoch:[53/150]\t loss=0.37531\t acc_hard=0.596 acc_soft=0.889 lr=0.0001668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:12:09,666][line: 93] ==> train Epoch:[54/150]\t loss=0.06297\t acc_hard=0.903 acc_soft=0.978 lr=0.0002543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:12:09.666120 139766070904640 3841376463.py:93] train Epoch:[54/150]\t loss=0.06297\t acc_hard=0.903 acc_soft=0.978 lr=0.0002543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:12:13,584][line: 93] ==> val Epoch:[54/150]\t loss=0.37348\t acc_hard=0.593 acc_soft=0.891 lr=0.0002543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:12:13.584882 139766070904640 3841376463.py:93] val Epoch:[54/150]\t loss=0.37348\t acc_hard=0.593 acc_soft=0.891 lr=0.0002543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:13:13,331][line: 93] ==> train Epoch:[55/150]\t loss=0.06134\t acc_hard=0.906 acc_soft=0.979 lr=0.0003606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:13:13.331681 139766070904640 3841376463.py:93] train Epoch:[55/150]\t loss=0.06134\t acc_hard=0.906 acc_soft=0.979 lr=0.0003606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:13:17,213][line: 93] ==> val Epoch:[55/150]\t loss=0.37014\t acc_hard=0.594 acc_soft=0.889 lr=0.0003606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:13:17.213491 139766070904640 3841376463.py:93] val Epoch:[55/150]\t loss=0.37014\t acc_hard=0.594 acc_soft=0.889 lr=0.0003606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:14:16,449][line: 93] ==> train Epoch:[56/150]\t loss=0.06050\t acc_hard=0.905 acc_soft=0.978 lr=0.0004852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:14:16.449522 139766070904640 3841376463.py:93] train Epoch:[56/150]\t loss=0.06050\t acc_hard=0.905 acc_soft=0.978 lr=0.0004852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:14:20,345][line: 93] ==> val Epoch:[56/150]\t loss=0.38124\t acc_hard=0.581 acc_soft=0.887 lr=0.0004852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:14:20.345672 139766070904640 3841376463.py:93] val Epoch:[56/150]\t loss=0.38124\t acc_hard=0.581 acc_soft=0.887 lr=0.0004852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:15:19,957][line: 93] ==> train Epoch:[57/150]\t loss=0.06039\t acc_hard=0.902 acc_soft=0.978 lr=0.0006276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:15:19.957569 139766070904640 3841376463.py:93] train Epoch:[57/150]\t loss=0.06039\t acc_hard=0.902 acc_soft=0.978 lr=0.0006276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-07 08:15:23,921][line: 93] ==> val Epoch:[57/150]\t loss=0.38560\t acc_hard=0.583 acc_soft=0.886 lr=0.0006276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1207 08:15:23.921538 139766070904640 3841376463.py:93] val Epoch:[57/150]\t loss=0.38560\t acc_hard=0.583 acc_soft=0.886 lr=0.0006276\n"
     ]
    }
   ],
   "source": [
    "n=1\n",
    "resume = True\n",
    "#0->RandomHorizontalFlip\n",
    "#1->RandomVerticalFlip\n",
    "#2->RandomRotation\n",
    "#3->RandomResizedCrop\n",
    "#4->RandomPerspective\n",
    "#5->GaussianBlur\n",
    "#6->RandomAdjustSharpness\n",
    "#7->random_select\n",
    "#8->normalize\n",
    "mode_list = [8]\n",
    "#mode_list = [2]#random_rotation\n",
    "#grid search \n",
    "for batch_size in [16]:\n",
    "    patch_size = 224\n",
    "    for mode in mode_list:\n",
    "        dataLoader = getDataSet(mode, patch_size, batch_size)\n",
    "        for lr in [(1.5*(1e-2),1.5*(1e-5))]:  \n",
    "            logger.info(\"experiment on:\"+ str(mode))     \n",
    "            root_dir = './' + str(mode) + '-' + str(mode) + '-' + str(mode)\n",
    "            if not os.path.isdir(root_dir):\n",
    "                os.mkdir(root_dir)\n",
    "            logger.info(\"batch_size:\" + str(batch_size))\n",
    "            logger.info(\"patch_size:\" + str(patch_size))\n",
    "            logger.info(\"learning rate high:\" + str(lr[0]))\n",
    "            logger.info(\"learning rate low:\" + str(lr[1]))\n",
    "            model = EfficientNet()\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model._initialize_weights()\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.to(device)\n",
    "            lr_h = lr[0]\n",
    "            lr_l = lr[1]\n",
    "            # do a step in grid search\n",
    "            \n",
    "            train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 150, resume=resume, \n",
    "    checkpoint = \"/checkpoint/ckpt_best_50.pth\", device = device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
