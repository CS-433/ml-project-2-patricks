{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a805e6b7",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "a805e6b7",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import crack_dataset as DS\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7576b5a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "f7576b5a",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [],
   "source": [
    "def getDataSet(patch_size, batch_size, workers=8):\n",
    "    class Args:\n",
    "      # dataset_path = \"/storage/data/classification_dataset_balanced/\"\n",
    "      dataset_path = \"../p2_data/data/classification_dataset_balanced/\"\n",
    "      patch_size = 1\n",
    "      batch_size = 1\n",
    "      workers = 1\n",
    "      def __init__(self, patch_size, batch_size, workers):\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "    args = Args(patch_size, batch_size, workers)\n",
    "    dataset = DS.CODEBRIM(torch.cuda.is_available(),args)\n",
    "    dataLoaders = {'train': dataset.train_loader, 'val': dataset.val_loader, 'test':dataset.test_loader}\n",
    "    return dataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e515a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(EfficientNet, self).__init__()\n",
    "#     self.model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained = False)\n",
    "    # load efficient net from torchhub\n",
    "    # self.model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet', type='efficientnet-widese-b0')\n",
    "    self.model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "\n",
    "    self.model.classifier[3] = nn.Linear(1280,6) #modify the output layer\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    x = torch.sigmoid(x) # the output from model should be fed into sigmoid to get the probability \n",
    "    return x\n",
    "  def _initialize_weights(self):\n",
    "    print(\"initialize parameters\")\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d):\n",
    "        #using kaiming's method to initialize convolution layer parameters as requested in the paper\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') \n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "      elif isinstance(m, nn.Linear):\n",
    "        #other parameters use normal distribution to initialize\n",
    "        nn.init.normal_(m.weight, 0, 0.01)  \n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_creater(output_dir):\n",
    "    \"\"\"\n",
    "    create logger object for registering staffs\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    log_name = '{}_{}.log'.format('efficientNet_pretrain' ,time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    final_log_file = os.path.join(output_dir,log_name)\n",
    " \n",
    " \n",
    "    # creat a log\n",
    "    log = logging.getLogger('train_log')\n",
    "    log.setLevel(logging.DEBUG)\n",
    " \n",
    "    # FileHandler\n",
    "    file = logging.FileHandler(final_log_file)\n",
    "    file.setLevel(logging.DEBUG)\n",
    " \n",
    "    # StreamHandler\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    " \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(asctime)s][line: %(lineno)d] ==> %(message)s')\n",
    " \n",
    "    # setFormatter\n",
    "    file.setFormatter(formatter)\n",
    "    stream.setFormatter(formatter)\n",
    "\n",
    "     # addHandler\n",
    "    log.addHandler(file)\n",
    "    log.addHandler(stream)\n",
    " \n",
    "    log.info('creating {}'.format(final_log_file))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs = 300, resume=False, \n",
    "    checkpoint = None, device = \"cpu\"):\n",
    "    start_epoch = 1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr_h, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=lr_l)\n",
    "    best_acc_hard = 0.0\n",
    "    best_acc_soft = 0.0\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    save_path_hard = root_dir + '/hard.pth'\n",
    "    save_path_soft = root_dir + '/soft.pth'\n",
    "    iters = len(dataLoader['train'])\n",
    "    if resume:\n",
    "        path_checkpoint = root_dir + checkpoint  # checkpoint path\n",
    "        checkpoint = torch.load(path_checkpoint)  # load the checkpoint\n",
    "        model.load_state_dict(checkpoint['net'])  # load the learnable params\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])  # load the params for optimizers\n",
    "        start_epoch = checkpoint['epoch']  # set the start epoch\n",
    "        best_acc_soft = checkpoint['best_acc_soft']\n",
    "        best_acc_hard = checkpoint['best_acc_hard']\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):  # loop over the dataset multiple times\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint = {\n",
    "            \"net\": model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc_soft\": best_acc_soft,\n",
    "            \"best_acc_hard\": best_acc_hard\n",
    "            }\n",
    "            if not os.path.isdir(root_dir + \"/checkpoint\"):\n",
    "                os.mkdir(root_dir + \"/checkpoint\")\n",
    "            torch.save(checkpoint, root_dir + '/checkpoint/ckpt_best_%s.pth' %(str(epoch)))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_hard = 0\n",
    "            running_corrects_soft = 0\n",
    "\n",
    "            for i, sample in enumerate(dataLoaders[phase]):\n",
    "                inputs, labels = sample\n",
    "                inputs = inputs.to(device)\n",
    "                if inputs.shape[0] < 2:  # avoid batch norm bug\n",
    "                    continue\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "                equality_matrix = (outputs.float() == labels).float()\n",
    "                hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "                soft = torch.mean(equality_matrix)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                #adjustment in scheduler\n",
    "                    scheduler.step(epoch + i / iters)\n",
    "        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_hard += hard.item()\n",
    "                running_corrects_soft += soft.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "            logger.info('{} Epoch:[{}/{}]\\t loss={:.5f}\\t acc_hard={:.3f} acc_soft={:.3f} lr={:.7f}'.format\\\n",
    "            (phase, epoch , num_epochs, epoch_loss, epoch_acc_hard, epoch_acc_soft, \\\n",
    "            optimizer.state_dict()['param_groups'][0]['lr'] ))\n",
    "\n",
    "            # deep copy the model\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_hard > best_acc_hard:\n",
    "                best_acc_hard = epoch_acc_hard\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_hard)\n",
    "\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_soft > best_acc_soft:\n",
    "                best_acc_soft = epoch_acc_soft\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_soft)\n",
    "\n",
    "    model = EfficientNet()\n",
    "    model.load_state_dict(torch.load(root_dir + '/hard.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"hard:\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "    model.load_state_dict(torch.load(root_dir + '/soft.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"soft:\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(dataLoaders, device, model, logger):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        running_loss = 0.0\n",
    "        running_corrects_hard = 0\n",
    "        running_corrects_soft = 0\n",
    "\n",
    "      \n",
    "        for i, sample in enumerate(dataLoaders[phase]):\n",
    "            inputs, labels = sample\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "            equality_matrix = (outputs.float() == labels).float()\n",
    "            hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "            soft = torch.mean(equality_matrix)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects_hard += hard.item()\n",
    "            running_corrects_soft += soft.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "        logger.info(\"{}: loss:{:.5f} acc_soft:{:.3f} acc_hard:{:.3f}\".format(phase, epoch_loss, epoch_acc_soft, epoch_acc_hard))     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-04 15:42:44,116][line: 35] ==> creating ./train_log/2021-12-04-15-42.log\n",
      "[2021-12-04 15:42:44,116][line: 35] ==> creating ./train_log/2021-12-04-15-42.log\n",
      "[2021-12-04 15:42:44,116][line: 35] ==> creating ./train_log/2021-12-04-15-42.log\n",
      "[2021-12-04 15:42:44,960][line: 12] ==> batch_size:4\n",
      "[2021-12-04 15:42:44,960][line: 12] ==> batch_size:4\n",
      "[2021-12-04 15:42:44,960][line: 12] ==> batch_size:4\n",
      "[2021-12-04 15:42:44,962][line: 13] ==> patch_size:224\n",
      "[2021-12-04 15:42:44,962][line: 13] ==> patch_size:224\n",
      "[2021-12-04 15:42:44,962][line: 13] ==> patch_size:224\n",
      "[2021-12-04 15:42:44,963][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-04 15:42:44,963][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-04 15:42:44,963][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-04 15:42:44,965][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-04 15:42:44,965][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-04 15:42:44,965][line: 15] ==> learning rate low:1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---debug use_se in SuperResIDWE1K7(16,48,2,72,1)\n",
      "---debug use_se in SuperResIDWE2K7(48,72,2,64,3)\n",
      "---debug use_se in SuperResIDWE2K7(72,152,2,144,3)\n",
      "---debug use_se in SuperResIDWE2K7(152,360,2,352,4)\n",
      "---debug use_se in SuperResIDWE4K7(360,288,1,264,3)\n",
      "model parameter number is: 17314938\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_2811/1724466735.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlr_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \n\u001b[0;32m---> 29\u001b[0;31m checkpoint = None, device = device)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_2811/1003609878.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs, resume, checkpoint, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0msoft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequality_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m#adjustment in scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logger = log_creater(\"./train_log\")\n",
    "batch_size = 4\n",
    "patch_size = 224\n",
    "dataLoader = getDataSet(patch_size, batch_size)\n",
    "\n",
    "lr = (1e-2,1e-5)\n",
    "root_dir = './' + str(batch_size) + '-' + str(patch_size) + '-' + str(lr[0])\n",
    "if not os.path.isdir(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "logger.info(\"batch_size:\" + str(batch_size))\n",
    "logger.info(\"patch_size:\" + str(patch_size))\n",
    "logger.info(\"learning rate high:\" + str(lr[0]))\n",
    "logger.info(\"learning rate low:\" + str(lr[1]))\n",
    "model = EfficientNet()\n",
    "\n",
    "# get the model parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'model parameter number is: {params}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "lr_h = lr[0]\n",
    "lr_l = lr[1]\n",
    "train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \n",
    "checkpoint = None, device = device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65514c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
