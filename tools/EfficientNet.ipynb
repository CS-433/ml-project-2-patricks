{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805e6b7",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "a805e6b7",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0a0+0aef44c\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datasets as DS\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7576b5a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "f7576b5a",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [],
   "source": [
    "def getDataSet(patch_size, batch_size, workers=8):\n",
    "    class Args:\n",
    "      dataset_path = \"/storage/data/classification_dataset_balanced/\"\n",
    "      patch_size = 1\n",
    "      batch_size = 1\n",
    "      workers = 1\n",
    "      def __init__(self, patch_size, batch_size, workers):\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "    args = Args(patch_size, batch_size, workers)\n",
    "    dataset = DS.CODEBRIM(torch.cuda.is_available(),args)\n",
    "    dataLoaders = {'train': dataset.train_loader, 'val': dataset.val_loader, 'test':dataset.test_loader}\n",
    "    return dataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b138a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "565b138a",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(EfficientNet, self).__init__()\n",
    "#     self.model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained = False)\n",
    "    self.model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet', type='efficientnet-widese-b0')\n",
    "    self.model.classifier[3] = nn.Linear(1280,6) #修改输出层\n",
    "  def forward(self, x):\n",
    "    x = self.model(x)\n",
    "    x = torch.sigmoid(x)\n",
    "    return x\n",
    "  def _initialize_weights(self):\n",
    "    print(\"initialize parameters\")\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "      elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)  #正态分布赋值\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_creater(output_dir):\r\n",
    "    if not os.path.exists(output_dir):\r\n",
    "        os.makedirs(output_dir)\r\n",
    "    log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\r\n",
    "    final_log_file = os.path.join(output_dir,log_name)\r\n",
    " \r\n",
    " \r\n",
    "    # creat a log\r\n",
    "    log = logging.getLogger('train_log')\r\n",
    "    log.setLevel(logging.DEBUG)\r\n",
    " \r\n",
    "    # FileHandler\r\n",
    "    file = logging.FileHandler(final_log_file)\r\n",
    "    file.setLevel(logging.DEBUG)\r\n",
    " \r\n",
    "    # StreamHandler\r\n",
    "    stream = logging.StreamHandler()\r\n",
    "    stream.setLevel(logging.DEBUG)\r\n",
    " \r\n",
    "    # Formatter\r\n",
    "    formatter = logging.Formatter(\r\n",
    "        '[%(asctime)s][line: %(lineno)d] ==> %(message)s')\r\n",
    " \r\n",
    "    # setFormatter\r\n",
    "    file.setFormatter(formatter)\r\n",
    "    stream.setFormatter(formatter)\r\n",
    "\r\n",
    "     # addHandler\r\n",
    "    log.addHandler(file)\r\n",
    "    log.addHandler(stream)\r\n",
    " \r\n",
    "    log.info('creating {}'.format(final_log_file))\r\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs = 300, resume=False, \r\n",
    "    checkpoint = None, device = \"cpu\"):\r\n",
    "    if resume:\r\n",
    "        path_checkpoint = root_dir + checkpoint  # 断点路径\r\n",
    "        checkpoint = torch.load(path_checkpoint)  # 加载断点\r\n",
    "        model.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\r\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\r\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数\r\n",
    "        start_epoch = checkpoint['epoch']  # 设置开始的epoch\r\n",
    "        best_acc_soft = checkpoint['best_acc_soft']\r\n",
    "        best_acc_hard = checkpoint['best_acc_hard']\r\n",
    "    else:\r\n",
    "        start_epoch = 1\r\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr_h, momentum=0.9)\r\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=lr_l)\r\n",
    "        best_acc_hard = 0.0\r\n",
    "        best_acc_soft = 0.0\r\n",
    "    criterion = torch.nn.BCELoss()\r\n",
    "    save_path_hard = root_dir + '/hard.pth'\r\n",
    "    save_path_soft = root_dir + '/soft.pth'\r\n",
    "    iters = len(dataLoader['train'])\r\n",
    "\r\n",
    "\r\n",
    "    for epoch in range(start_epoch, num_epochs+1):  # loop over the dataset multiple times\r\n",
    "\r\n",
    "        if epoch % 20 == 0:\r\n",
    "            checkpoint = {\r\n",
    "            \"net\": model.state_dict(),\r\n",
    "            'optimizer': optimizer.state_dict(),\r\n",
    "            'scheduler': scheduler.state_dict(),\r\n",
    "            \"epoch\": epoch,\r\n",
    "            \"best_acc_soft\": best_acc_soft,\r\n",
    "            \"best_acc_hard\": best_acc_hard\r\n",
    "            }\r\n",
    "            if not os.path.isdir(root_dir + \"/checkpoint\"):\r\n",
    "                os.mkdir(root_dir + \"/checkpoint\")\r\n",
    "            torch.save(checkpoint, root_dir + '/checkpoint/ckpt_best_%s.pth' %(str(epoch)))\r\n",
    "\r\n",
    "        for phase in ['train', 'val']:\r\n",
    "            if phase == 'train':\r\n",
    "                model.train()  # Set model to training mode\r\n",
    "            else:\r\n",
    "                model.eval()   # Set model to evaluate mode\r\n",
    "\r\n",
    "            running_loss = 0.0\r\n",
    "            running_corrects_hard = 0\r\n",
    "            running_corrects_soft = 0\r\n",
    "\r\n",
    "            for i, sample in enumerate(dataLoaders[phase]):\r\n",
    "                inputs, labels = sample\r\n",
    "                inputs = inputs.to(device)\r\n",
    "                labels = labels.to(device)\r\n",
    "                # zero the parameter gradients\r\n",
    "                optimizer.zero_grad()\r\n",
    "\r\n",
    "                # forward + backward + optimize\r\n",
    "                outputs = model(inputs)\r\n",
    "                loss = criterion(outputs, labels)\r\n",
    "\r\n",
    "                outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\r\n",
    "                equality_matrix = (outputs.float() == labels).float()\r\n",
    "                hard = torch.sum(torch.prod(equality_matrix, dim=1))\r\n",
    "                soft = torch.mean(equality_matrix)\r\n",
    "                if phase == 'train':\r\n",
    "                    loss.backward()\r\n",
    "                    optimizer.step()\r\n",
    "                #adjustment in scheduler\r\n",
    "                    scheduler.step(epoch + i / iters)\r\n",
    "        \r\n",
    "                running_loss += loss.item() * inputs.size(0)\r\n",
    "                running_corrects_hard += hard.item()\r\n",
    "                running_corrects_soft += soft.item()\r\n",
    "\r\n",
    "            epoch_loss = running_loss / len(dataLoaders[phase].dataset)\r\n",
    "            epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\r\n",
    "            epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\r\n",
    "            logger.info('{} Epoch:[{}/{}]\\t loss={:.5f}\\t acc_hard={:.3f} acc_soft={:.3f} lr={:.7f}'.format\\\r\n",
    "            (phase, epoch , num_epochs, epoch_loss, epoch_acc_hard, epoch_acc_soft, \\\r\n",
    "            optimizer.state_dict()['param_groups'][0]['lr'] ))\r\n",
    "\r\n",
    "            # deep copy the model\r\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_hard > best_acc_hard:\r\n",
    "                best_acc_hard = epoch_acc_hard\r\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\r\n",
    "                torch.save(model.state_dict(), save_path_hard)\r\n",
    "\r\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_soft > best_acc_soft:\r\n",
    "                best_acc_soft = epoch_acc_soft\r\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\r\n",
    "                torch.save(model.state_dict(), save_path_soft)\r\n",
    "\r\n",
    "    model = EfficientNet()\r\n",
    "    model.load_state_dict(torch.load(root_dir + '/hard.pth'))\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    logger.info(\"hard:\")\r\n",
    "    evaluation(dataLoaders, device, model, logger)\r\n",
    "\r\n",
    "    model.load_state_dict(torch.load(root_dir + '/soft.pth'))\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    logger.info(\"soft:\")\r\n",
    "    evaluation(dataLoaders, device, model, logger)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def evaluation(dataLoaders, device, model, logger):\r\n",
    "    for phase in ['train', 'val', 'test']:\r\n",
    "        running_loss = 0.0\r\n",
    "        running_corrects_hard = 0\r\n",
    "        running_corrects_soft = 0\r\n",
    "\r\n",
    "      \r\n",
    "        for i, sample in enumerate(dataLoaders[phase]):\r\n",
    "            inputs, labels = sample\r\n",
    "            inputs = inputs.to(device)\r\n",
    "            labels = labels.to(device)\r\n",
    "            outputs = model(inputs)\r\n",
    "            loss = criterion(outputs, labels)\r\n",
    "\r\n",
    "            outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\r\n",
    "            equality_matrix = (outputs.float() == labels).float()\r\n",
    "            hard = torch.sum(torch.prod(equality_matrix, dim=1))\r\n",
    "            soft = torch.mean(equality_matrix)\r\n",
    "            running_loss += loss.item() * inputs.size(0)\r\n",
    "            running_corrects_hard += hard.item()\r\n",
    "            running_corrects_soft += soft.item()\r\n",
    "\r\n",
    "        epoch_loss = running_loss / len(dataLoaders[phase].dataset)\r\n",
    "        epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\r\n",
    "        epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\r\n",
    "        logger.info(\"{}: loss:{:.5f} acc_soft:{:.3f} acc_hard:{:.3f}\".format(phase, epoch_loss, epoch_acc_soft, epoch_acc_hard))     \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-29 08:52:37,964][line: 32] ==> creating ./train_log/2021-11-29-08-52.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-29 08:52:38,716][line: 13] ==> batch_size:16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-29 08:52:38,717][line: 14] ==> patch_size:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-29 08:52:38,718][line: 15] ==> learning rate high:0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-29 08:52:38,718][line: 16] ==> learning rate low:0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/archive/torchhub.zip\" to /root/.cache/torch/hub/torchhub.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/efficientnet_widese_b0_pyt_amp/versions/20.12.0/files/nvidia_efficientnet-widese-b0_210412.pth\" to /root/.cache/torch/hub/checkpoints/nvidia_efficientnet-widese-b0_210412.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c528a9e862d04a5da8dd5ef6e69caa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/32.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize parameters\n"
     ]
    }
   ],
   "source": [
    "n=1\r\n",
    "logger = log_creater(\"./train_log\")\r\n",
    "for batch_size in [16,32]:\r\n",
    "    for patch_size in [32,64]:\r\n",
    "        dataLoader = getDataSet(patch_size, batch_size)\r\n",
    "        for lr in [(1e-1,1e-5), (5e-2,5e-4),(1e-2,1e-5)]:\r\n",
    "            root_dir = './' + str(batch_size) + '-' + str(patch_size) + '-' + str(lr[0])\r\n",
    "            if not os.path.isdir(root_dir):\r\n",
    "                os.mkdir(root_dir)\r\n",
    "            if n<=1:\r\n",
    "                n=n+1\r\n",
    "                continue\r\n",
    "            logger.info(\"batch_size:\" + str(batch_size))\r\n",
    "            logger.info(\"patch_size:\" + str(patch_size))\r\n",
    "            logger.info(\"learning rate high:\" + str(lr[0]))\r\n",
    "            logger.info(\"learning rate low:\" + str(lr[1]))\r\n",
    "            model = EfficientNet()\r\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "            model._initialize_weights()\r\n",
    "            if torch.cuda.is_available():\r\n",
    "                model = model.to(device)\r\n",
    "            lr_h = lr[0]\r\n",
    "            lr_l = lr[1]\r\n",
    "            train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \r\n",
    "    checkpoint = None, device = device)\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a887b9-3e11-4135-a18e-1859fb0286b3",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "11a887b9-3e11-4135-a18e-1859fb0286b3",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
