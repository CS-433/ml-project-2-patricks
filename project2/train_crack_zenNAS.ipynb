{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3d2aff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a805e6b7",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "a805e6b7",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import crack_dataset as DS\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from tools.focal_loss import focal_binary_cross_entropy\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7576b5a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "f7576b5a",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [],
   "source": [
    "def getDataSet(patch_size, batch_size, workers=8):\n",
    "    class Args:\n",
    "      # dataset_path = \"/storage/data/classification_dataset_balanced/\"\n",
    "      dataset_path = \"../p2_data/data/classification_dataset_balanced/\"\n",
    "      patch_size = 1\n",
    "      batch_size = 1\n",
    "      workers = 1\n",
    "      def __init__(self, patch_size, batch_size, workers):\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "    args = Args(patch_size, batch_size, workers)\n",
    "    dataset = DS.CODEBRIM(torch.cuda.is_available(),args)\n",
    "    dataLoaders = {'train': dataset.train_loader, 'val': dataset.val_loader, 'test':dataset.test_loader}\n",
    "    return dataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e515a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zennet(arch_path, num_classes, use_SE):\n",
    "    \"\"\"\n",
    "    load the Zen-NAS searched model from stored arch planetext\n",
    "\n",
    "    :param arch_path: path for model architecture description file.txt\n",
    "    :param num_classes: the data class number\n",
    "    :param use_SE: whether to use Squeeze-and-Excitation module\n",
    "    \"\"\"\n",
    "    from ref_codes.ZenNAS.ZenNet import masternet\n",
    "    \n",
    "    with open(arch_path, 'r') as fid:\n",
    "        model_plainnet_str = fid.readline().strip()\n",
    "    \n",
    "    model = masternet.PlainNet(num_classes=num_classes, plainnet_struct=model_plainnet_str, use_se=use_SE)\n",
    "    return model\n",
    "\n",
    "def get_ZenNet_pretrained(model_name, num_classes):\n",
    "    from ref_codes.ZenNAS.ZenNet import get_ZenNet\n",
    "    from ref_codes.ZenNAS.PlainNet import basic_blocks\n",
    "    model = get_ZenNet(model_name, pretrained=True)\n",
    "    \n",
    "    # adjust the last layer to adapt to the new class number\n",
    "    model.fc_linear = basic_blocks.Linear(in_channels=model.fc_linear.in_channels, out_channels=num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_creater(output_dir):\n",
    "    \"\"\"\n",
    "    create logger object for registering staffs\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    final_log_file = os.path.join(output_dir,log_name)\n",
    " \n",
    " \n",
    "    # creat a log\n",
    "    log = logging.getLogger('train_log')\n",
    "    log.setLevel(logging.DEBUG)\n",
    " \n",
    "    # FileHandler\n",
    "    file = logging.FileHandler(final_log_file)\n",
    "    file.setLevel(logging.DEBUG)\n",
    " \n",
    "    # StreamHandler\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    " \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(asctime)s][line: %(lineno)d] ==> %(message)s')\n",
    " \n",
    "    # setFormatter\n",
    "    file.setFormatter(formatter)\n",
    "    stream.setFormatter(formatter)\n",
    "\n",
    "     # addHandler\n",
    "    log.addHandler(file)\n",
    "    log.addHandler(stream)\n",
    " \n",
    "    log.info('creating {}'.format(final_log_file))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs = 300, resume=False, \n",
    "    checkpoint = None, device = \"cpu\"):\n",
    "    start_epoch = 1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr_h, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=lr_l)\n",
    "    best_acc_hard = 0.0\n",
    "    best_acc_soft = 0.0\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0,0.9854830961388338,1.5000748374861417,5.214233129729914,2.1126041872854255,2.0173556568167372], device=torch.device('cuda')))  \n",
    "    \n",
    "    save_path_hard = root_dir + '/hard.pth'\n",
    "    save_path_soft = root_dir + '/soft.pth'\n",
    "    iters = len(dataLoader['train'])\n",
    "    if resume:\n",
    "        path_checkpoint = root_dir + checkpoint  # checkpoint path\n",
    "        checkpoint = torch.load(path_checkpoint)  # load the checkpoint\n",
    "        model.load_state_dict(checkpoint['net'])  # load the learnable params\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])  # load the params for optimizers\n",
    "        start_epoch = checkpoint['epoch']  # set the start epoch\n",
    "        best_acc_soft = checkpoint['best_acc_soft']\n",
    "        best_acc_hard = checkpoint['best_acc_hard']\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):  # loop over the dataset multiple times\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint = {\n",
    "            \"net\": model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc_soft\": best_acc_soft,\n",
    "            \"best_acc_hard\": best_acc_hard\n",
    "            }\n",
    "            if not os.path.isdir(root_dir + \"/checkpoint\"):\n",
    "                os.mkdir(root_dir + \"/checkpoint\")\n",
    "            torch.save(checkpoint, root_dir + '/checkpoint/ckpt_best_%s.pth' %(str(epoch)))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_hard = 0\n",
    "            running_corrects_soft = 0\n",
    "\n",
    "            for i, sample in enumerate(dataLoaders[phase]):\n",
    "                inputs, labels = sample\n",
    "                inputs = inputs.to(device)\n",
    "                if inputs.shape[0] < 2:  # avoid batch norm bug\n",
    "                    continue\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                #== original loss\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                #== BCE loss with logits\n",
    "                # loss = criterion(outputs, labels)\n",
    "                # outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "                #== focal loss\n",
    "                # loss = focal_binary_cross_entropy(outputs, labels)  # multi-label focal loss\n",
    "                # outputs = torch.sigmoid(outputs)\n",
    "\n",
    "                outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "                equality_matrix = (outputs.float() == labels).float()\n",
    "                hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "                soft = torch.mean(equality_matrix)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                #adjustment in scheduler\n",
    "                    scheduler.step(epoch + i / iters)\n",
    "        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_hard += hard.item()\n",
    "                running_corrects_soft += soft.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "            logger.info('{} Epoch:[{}/{}]\\t loss={:.5f}\\t acc_hard={:.3f} acc_soft={:.3f} lr={:.7f}'.format\\\n",
    "            (phase, epoch , num_epochs, epoch_loss, epoch_acc_hard, epoch_acc_soft, \\\n",
    "            optimizer.state_dict()['param_groups'][0]['lr'] ))\n",
    "\n",
    "            # deep copy the model\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_hard > best_acc_hard:\n",
    "                best_acc_hard = epoch_acc_hard\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_hard)\n",
    "\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_soft > best_acc_soft:\n",
    "                best_acc_soft = epoch_acc_soft\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_soft)\n",
    "\n",
    "    # model = get_zennet('model_scripts/zennet_imagenet1k_flops400M_res224.txt', 6, True)\n",
    "    model = get_ZenNet_pretrained('zennet_imagenet1k_flops400M_SE_res224')\n",
    "    \n",
    "    model.load_state_dict(torch.load(root_dir + '/hard.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"hard:\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "    model.load_state_dict(torch.load(root_dir + '/soft.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"soft:\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(dataLoaders, device, model, logger):\n",
    "    # criterion = torch.nn.BCELoss()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0,0.9854830961388338,1.5000748374861417,5.214233129729914,2.1126041872854255,2.0173556568167372], device=torch.device('cuda')))  \n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        running_loss = 0.0\n",
    "        running_corrects_hard = 0\n",
    "        running_corrects_soft = 0\n",
    "      \n",
    "        for i, sample in enumerate(dataLoaders[phase]):\n",
    "            inputs, labels = sample\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            #== original loss\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "             \n",
    "            #== BCE loss with logits\n",
    "            # loss = criterion(outputs, labels)\n",
    "            # outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            #== focal loss\n",
    "            # loss = focal_binary_cross_entropy(outputs, labels)  # multi-label focal loss\n",
    "            # outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "            equality_matrix = (outputs.float() == labels).float()\n",
    "            hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "            soft = torch.mean(equality_matrix)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects_hard += hard.item()\n",
    "            running_corrects_soft += soft.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "        logger.info(\"{}: loss:{:.5f} acc_soft:{:.3f} acc_hard:{:.3f}\".format(phase, epoch_loss, epoch_acc_soft, epoch_acc_hard))     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:04:59,774][line: 35] ==> creating ./train_log/2021-12-09-11-04.log\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,688][line: 12] ==> batch_size:4\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,693][line: 13] ==> patch_size:224\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,697][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-09 11:05:00,701][line: 15] ==> learning rate low:1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---debug use_se in SuperResIDWE1K7(24,48,2,48,1)\n",
      "---debug use_se in SuperResIDWE2K7(48,72,2,72,1)\n",
      "---debug use_se in SuperResIDWE6K7(72,96,2,88,5)\n",
      "---debug use_se in SuperResIDWE4K7(96,192,2,168,5)\n",
      "model parameter number is: 9274958\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 93726, 93728, 93729, 93730, 93731, 93732, 93733, 93734) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_2811/2513037751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlr_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \n\u001b[0;32m---> 29\u001b[0;31m checkpoint = None, device = device)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_2811/4064416421.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs, resume, checkpoint, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mrunning_corrects_soft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataLoaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 93726, 93728, 93729, 93730, 93731, 93732, 93733, 93734) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logger = log_creater(\"./train_log\")\n",
    "batch_size = 4\n",
    "patch_size = 224\n",
    "dataLoader = getDataSet(patch_size, batch_size)\n",
    "\n",
    "lr = (1e-2,1e-5)\n",
    "root_dir = './' + str(batch_size) + '-' + str(patch_size) + '-' + str(lr[0])\n",
    "if not os.path.isdir(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "logger.info(\"batch_size:\" + str(batch_size))\n",
    "logger.info(\"patch_size:\" + str(patch_size))\n",
    "logger.info(\"learning rate high:\" + str(lr[0]))\n",
    "logger.info(\"learning rate low:\" + str(lr[1]))\n",
    "# model = get_zennet('model_scripts/zennet_imagenet1k_flops400M_res224.txt', 6, True)\n",
    "model = get_ZenNet_pretrained('zennet_imagenet1k_flops400M_SE_res224')\n",
    "\n",
    "# get the model parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'model parameter number is: {params}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "lr_h = lr[0]\n",
    "lr_l = lr[1]\n",
    "train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \n",
    "checkpoint = None, device = device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65514c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
