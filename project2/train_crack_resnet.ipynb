{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a805e6b7",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "a805e6b7",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import crack_dataset as DS\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7576b5a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "f7576b5a",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [],
   "source": [
    "def getDataSet(patch_size, batch_size, workers=8):\n",
    "    class Args:\n",
    "      # dataset_path = \"/storage/data/classification_dataset_balanced/\"\n",
    "      dataset_path = \"../p2_data/data/classification_dataset_balanced/\"\n",
    "      patch_size = 1\n",
    "      batch_size = 1\n",
    "      workers = 1\n",
    "      def __init__(self, patch_size, batch_size, workers):\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "    args = Args(patch_size, batch_size, workers)\n",
    "    dataset = DS.CODEBRIM(torch.cuda.is_available(),args)\n",
    "    dataLoaders = {'train': dataset.train_loader, 'val': dataset.val_loader, 'test':dataset.test_loader}\n",
    "    return dataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565b138a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "565b138a",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57"
    }
   },
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "  \"\"\"\n",
    "  Resnet model class modified for multi-target Crack dataset\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(Resnet, self).__init__()\n",
    "    self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)\n",
    "    self.model.fc = nn.Linear(2048,6) # modify the output layer\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    forward step\n",
    "    \"\"\"\n",
    "    x = self.model(x)\n",
    "    x = torch.sigmoid(x)  # for binary output\n",
    "    return x\n",
    "  def _initialize_weights(self):\n",
    "    \"\"\"\n",
    "    initialzie the parameters\n",
    "    \"\"\"\n",
    "    print(\"initialize parameters\")\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "      elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)  # initialize with normal distribution\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def get_resnet50(num_class=6, pretrained=False):\n",
    "    import torchvision.models as models\n",
    "    # load vgg net from torchhub\n",
    "    model = models.resnet50(pretrained=pretrained)  # Resnet-50\n",
    "    model.fc = nn.Linear(in_features=model.fc.in_features, out_features=num_class, bias=True) # modify the output layer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "be5c39b3-eee5-4fc9-97f2-790ed233205d",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def log_creater(output_dir):\n",
    "    \"\"\"\n",
    "    create logger object for registering staffs\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    final_log_file = os.path.join(output_dir,log_name)\n",
    " \n",
    " \n",
    "    # creat a log\n",
    "    log = logging.getLogger('train_log')\n",
    "    log.setLevel(logging.DEBUG)\n",
    " \n",
    "    # FileHandler\n",
    "    file = logging.FileHandler(final_log_file)\n",
    "    file.setLevel(logging.DEBUG)\n",
    " \n",
    "    # StreamHandler\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    " \n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(asctime)s][line: %(lineno)d] ==> %(message)s')\n",
    " \n",
    "    # setFormatter\n",
    "    file.setFormatter(formatter)\n",
    "    stream.setFormatter(formatter)\n",
    "\n",
    "     # addHandler\n",
    "    log.addHandler(file)\n",
    "    log.addHandler(stream)\n",
    " \n",
    "    log.info('creating {}'.format(final_log_file))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "1aa3e7d7-70e0-4364-9d3c-358600a13727",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs = 300, resume=False, \n",
    "    checkpoint = None, device = \"cpu\"):\n",
    "    start_epoch = 1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr_h, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10, T_mult=2, eta_min=lr_l)\n",
    "    best_acc_hard = 0.0\n",
    "    best_acc_soft = 0.0\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    save_path_hard = root_dir + '/hard.pth'\n",
    "    save_path_soft = root_dir + '/soft.pth'\n",
    "    iters = len(dataLoader['train'])\n",
    "    if resume:\n",
    "        path_checkpoint = root_dir + checkpoint  # checkpoint path\n",
    "        checkpoint = torch.load(path_checkpoint)  # load the checkpoint\n",
    "        model.load_state_dict(checkpoint['net'])  # load the learnable params\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])  # load the params for optimizers\n",
    "        start_epoch = checkpoint['epoch']  # set the start epoch\n",
    "        best_acc_soft = checkpoint['best_acc_soft']\n",
    "        best_acc_hard = checkpoint['best_acc_hard']\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):  # loop over the dataset multiple times\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            checkpoint = {\n",
    "            \"net\": model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc_soft\": best_acc_soft,\n",
    "            \"best_acc_hard\": best_acc_hard\n",
    "            }\n",
    "            if not os.path.isdir(root_dir + \"/resnetWpretrain_checkpoint\"):\n",
    "                os.mkdir(root_dir + \"/resnetWpretrain_checkpoint\")\n",
    "            torch.save(checkpoint, root_dir + '/resnetWpretrain_checkpoint/ckpt_best_%s.pth' %(str(epoch)))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_hard = 0\n",
    "            running_corrects_soft = 0\n",
    "\n",
    "            for i, sample in enumerate(dataLoaders[phase]):\n",
    "                inputs, labels = sample\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "                equality_matrix = (outputs.float() == labels).float()\n",
    "                hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "                soft = torch.mean(equality_matrix)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                #adjustment in scheduler\n",
    "                    scheduler.step(epoch + i / iters)\n",
    "        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_hard += hard.item()\n",
    "                running_corrects_soft += soft.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "            epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "            logger.info('{} Epoch:[{}/{}]\\t loss={:.5f}\\t acc_hard={:.3f} acc_soft={:.3f} lr={:.7f}'.format\\\n",
    "            (phase, epoch , num_epochs, epoch_loss, epoch_acc_hard, epoch_acc_soft, \\\n",
    "            optimizer.state_dict()['param_groups'][0]['lr'] ))\n",
    "\n",
    "            # deep copy the model\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_hard > best_acc_hard:\n",
    "                best_acc_hard = epoch_acc_hard\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_hard)\n",
    "\n",
    "            if epoch >= 150 and phase == 'val' and epoch_acc_soft > best_acc_soft:\n",
    "                best_acc_soft = epoch_acc_soft\n",
    "                #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_path_soft)\n",
    "\n",
    "    model = get_resnet50(num_class=6, pretrained=True)\n",
    "    model.load_state_dict(torch.load(root_dir + '/hard.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"hard:\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "    model.load_state_dict(torch.load(root_dir + '/soft.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"soft:\")\n",
    "    evaluation(dataLoaders, device, model, logger)\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(dataLoaders, device, model, logger):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        running_loss = 0.0\n",
    "        running_corrects_hard = 0\n",
    "        running_corrects_soft = 0\n",
    "\n",
    "      \n",
    "        for i, sample in enumerate(dataLoaders[phase]):\n",
    "            inputs, labels = sample\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            outputs = outputs >= 0.5  # binarizing sigmoid output by thresholding with 0.5\n",
    "            equality_matrix = (outputs.float() == labels).float()\n",
    "            hard = torch.sum(torch.prod(equality_matrix, dim=1))\n",
    "            soft = torch.mean(equality_matrix)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects_hard += hard.item()\n",
    "            running_corrects_soft += soft.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_hard = running_corrects_hard / len(dataLoaders[phase].dataset)\n",
    "        epoch_acc_soft = running_corrects_soft / len(dataLoaders[phase])\n",
    "        logger.info(\"{}: loss:{:.5f} acc_soft:{:.3f} acc_hard:{:.3f}\".format(phase, epoch_loss, epoch_acc_soft, epoch_acc_hard))     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "41e6f8fe-436e-45d3-8476-846c196e3906",
     "kernelId": "6abd9485-bdd5-4477-bbbc-cec01aae3a57",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-04 10:05:54,427][line: 35] ==> creating ./train_log/2021-12-04-10-05.log\n",
      "[2021-12-04 10:05:54,427][line: 35] ==> creating ./train_log/2021-12-04-10-05.log\n",
      "[2021-12-04 10:05:55,126][line: 12] ==> batch_size:4\n",
      "[2021-12-04 10:05:55,126][line: 12] ==> batch_size:4\n",
      "[2021-12-04 10:05:55,127][line: 13] ==> patch_size:224\n",
      "[2021-12-04 10:05:55,127][line: 13] ==> patch_size:224\n",
      "[2021-12-04 10:05:55,129][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-04 10:05:55,129][line: 14] ==> learning rate high:0.01\n",
      "[2021-12-04 10:05:55,130][line: 15] ==> learning rate low:1e-05\n",
      "[2021-12-04 10:05:55,130][line: 15] ==> learning rate low:1e-05\n",
      "Using cache found in /Users/elenath/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter number is: 23520326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_1972/3492992071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlr_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \n\u001b[0;32m---> 29\u001b[0;31m checkpoint = None, device = device)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_1972/2868600409.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(root_dir, model, logger, lr_h, lr_l, dataLoaders, num_epochs, resume, checkpoint, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vw/gh508_4952v9wym_b397btvh0000gn/T/ipykernel_1972/2223130852.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mforward\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for binary output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/EPFL/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logger = log_creater(\"./resnetWpretrain_log\")\n",
    "batch_size = 16\n",
    "patch_size = 224\n",
    "dataLoader = getDataSet(patch_size, batch_size)\n",
    "\n",
    "lr = (1e-2,1e-5)\n",
    "root_dir = './' + str(batch_size) + '-' + str(patch_size) + '-' + str(lr[0])\n",
    "if not os.path.isdir(root_dir):\n",
    "    os.mkdir(root_dir)\n",
    "logger.info(\"batch_size:\" + str(batch_size))\n",
    "logger.info(\"patch_size:\" + str(patch_size))\n",
    "logger.info(\"learning rate high:\" + str(lr[0]))\n",
    "logger.info(\"learning rate low:\" + str(lr[1]))\n",
    "model = get_resnet50(num_class=6, pretrained=True)\n",
    "\n",
    "# get the model parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'model parameter number is: {params}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "lr_h = lr[0]\n",
    "lr_l = lr[1]\n",
    "train(root_dir, model, logger, lr_h, lr_l, dataLoader, num_epochs = 300, resume=False, \n",
    "checkpoint = None, device = device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
